{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72f1b162",
   "metadata": {},
   "source": [
    "## Pipeline with categorical and numerical features\n",
    "\n",
    "In this notebook we present complete pipelines, which chain together data preprocessing and Machine Learning models.\n",
    "We will use the `auto-mpg-orig` dataset, which has missing values (in the `hp` feature), numerical features and categorical features.\n",
    "\n",
    "Our data preprocessing must:\n",
    "1. Impute missing data in the numerical columns. Because in our example dataset we know that the only missing data is in column `hp` and this column is numerical, we can limit our imputation to numerical columns. In the general case we could also do imputation on categorical columns. To impute data using the column median, we use sklearn's `SimpleImputer` with `strategy='median'`. For a categorical column, to use the mode, I would use `strategy='most_frequent'`. For more advanced imputation techniques, one could use `IterativeImputer` or `KNNImputer`.\n",
    "2. One-hot-encode categorical columns. To this end we use sklearn's `OneHotEncoder`.\n",
    "3. Standardise numerical columns. We use `StandardScaler`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c394e",
   "metadata": {},
   "source": [
    "### Read data, mark missing values and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52536994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28ee4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('auto-mpg-orig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acfba870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform '?' in column hp as NaN\n",
    "d.hp = pd.to_numeric(d.hp, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8273ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark 'origin' as a categorical column, with the correct category names\n",
    "d.origin = pd.Categorical(d.origin.replace({1: 'America', 2: 'Europe', 3: 'Japan'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee7c3d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'mpg'\n",
    "features = [col for col in d.columns if col != label]\n",
    "\n",
    "# On top of the list of all features, we now also want\n",
    "# the list of categorical features (only one in our example)\n",
    "# and the list of numerical features (those which are not categorical)\n",
    "categorical_features = ['origin']\n",
    "numerical_features = [col for col in features if col not in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db3b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7899bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = d[features], d[label]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86decc",
   "metadata": {},
   "source": [
    "## Create the preprocessing step of the pipeline\n",
    "\n",
    "Because preprocessing is composed of many steps, we can use pipelines to create a preprocessor.\n",
    "Later on, we will create a pipeline which joins together the preprocessor and the Machine Learning model (so we will be creating a pipeline in which one of the steps is... another pipeline!).\n",
    "\n",
    "This approach is useful because, in our example, preprocessing is going to be the same no matter what Machine Learning model we use.\n",
    "Therefore, by creating a generic preprocessor, we can then \"recycle\" it and use it in combination to whatever ML model we want.\n",
    "\n",
    "When we must take different actions on different columns we must use a `ColumnTransformer`.\n",
    "Examples of this situation are:\n",
    "* one-hot-encode *only* categorical columns (and to nothing to numerical columns);\n",
    "* impute *only* numerical columns (and do nothing to categorical columns); \n",
    "* standardise *only* numerical columns.\n",
    "\n",
    "A column transformer object takes a list of tuples.\n",
    "Each tuple has three elements:\n",
    "1. The first one is just a name to describe it (i.e., `'numerical'` and `'categorical'` in the code below).\n",
    "2. The second one is the actual object which does the preprocessing (i.e., the pipelines in the code below).\n",
    "3. The third one is a subset of columns on which we want to apply the preprocessing (i.e., `numerical_columns` and `categorical_columns` in the code below).\n",
    "\n",
    "It is important to note that the subsets of columns we pass to the various tuples (as their third element) must partition the entire set of features.\n",
    "In other words, the same column cannot appear in more than one subset and each column must appear in at least one subset.\n",
    "In our case, this means that `numerical_columns` and `categorical_columns` cannot share common elements, and there cannot be any column which is neiter in `numerical_columns` nor in `categorical_columns`.\n",
    "\n",
    "If a column never appears in any of the column subsets, it will be dropped from the set of features!\n",
    "Then, it's natural to ask: what if don't want to apply any preprocessing to some columns?\n",
    "If I simply exclude them from any subset of columns, they will be dropped.\n",
    "The answer is to add them to their own set and, when adding the corresponding tuple to `ColumnTransformer`, instead of passing a preprocessor as the second element, simply pass the string `'passthrough'`.\n",
    "This is a special value which means: just leave these columns alone and don't touch them.\n",
    "For example, if I don't want to 1-hot-encode categorical features, I can modify the code below and replace `('categorical', categorical_preprocessor, categorical_features)` with `('categorical', 'passthrough', categorical_features)`.\n",
    "\n",
    "Finally, let's spend a couple of words about the preprocessors themselves.\n",
    "I create two of them, one for numerical columns (`numerical_preprocessor`) and one for categorical columns (`categorical_preprocessor`).\n",
    "As already mentioned, because preprocessing each of these two subsets of columns could involve multiple steps, it is reasonable to use a pipeline to create the preprocessors.\n",
    "The numerical preprocessor first performs data imputation (using the column median for columns which have missing values) and then standardisation.\n",
    "The categorical preprocessor only performs 1-hot-encoding (I passed `sparse=False` to tell `numpy` not to use sparse matrices when encoding, because sklearn's Machine Learning models do not like them!) in this particular example but, in principle, I oculd be dealing with data in which imputation is required for categorical columns, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c70846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09f7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_preprocessor = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "categorical_preprocessor = make_pipeline(\n",
    "    OneHotEncoder(sparse=False)\n",
    ")\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('numerical', numerical_preprocessor, numerical_features),\n",
    "    ('categorical', categorical_preprocessor, categorical_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59984393",
   "metadata": {},
   "source": [
    "### Combining preprocessing and the models\n",
    "\n",
    "For this example I am using two linear models with, respectively, Lasso and Ridge regulatisation terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae80488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(\n",
    "    preprocessing,\n",
    "    Lasso(max_iter=10000))\n",
    "\n",
    "ridge = make_pipeline(\n",
    "    preprocessing,\n",
    "    Ridge())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd193fe",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning via grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95381c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_alpha = np.logspace(start=-3, stop=0, num=20)\n",
    "ridge_alpha = np.logspace(start=-1, stop=2, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0119b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv = GridSearchCV(\n",
    "    estimator=lasso,\n",
    "    param_grid={\n",
    "        'lasso__alpha': lasso_alpha\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error')\n",
    "\n",
    "ridge_cv = GridSearchCV(\n",
    "    estimator=ridge,\n",
    "    param_grid={\n",
    "        'ridge__alpha': ridge_alpha\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af905983",
   "metadata": {},
   "source": [
    "### Fitting of the estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2cbc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('numerical',\n",
       "                                                                         Pipeline(steps=[('simpleimputer',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('standardscaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['cylinders',\n",
       "                                                                          'displacement',\n",
       "                                                                          'hp',\n",
       "                                                                          'weight',\n",
       "                                                                          'acceleration',\n",
       "                                                                          'year']),\n",
       "                                                                        ('categorical',\n",
       "                                                                         Pipeline(steps=[('onehotencoder',\n",
       "                                                                                          OneHotEncoder(sparse=False))]),\n",
       "                                                                         ['origin'])])),\n",
       "                                       ('lasso', Lasso(max_iter=10000))]),\n",
       "             param_grid={'lasso__alpha': array([0.001     , 0.00143845, 0.00206914, 0.00297635, 0.00428133,\n",
       "       0.00615848, 0.00885867, 0.01274275, 0.01832981, 0.02636651,\n",
       "       0.0379269 , 0.05455595, 0.078476  , 0.11288379, 0.16237767,\n",
       "       0.23357215, 0.33598183, 0.48329302, 0.6951928 , 1.        ])},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adcc40db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lasso__alpha': 0.0379269019073225}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa17886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('numerical',\n",
       "                                                                         Pipeline(steps=[('simpleimputer',\n",
       "                                                                                          SimpleImputer(strategy='median')),\n",
       "                                                                                         ('standardscaler',\n",
       "                                                                                          StandardScaler())]),\n",
       "                                                                         ['cylinders',\n",
       "                                                                          'displacement',\n",
       "                                                                          'hp',\n",
       "                                                                          'weight',\n",
       "                                                                          'acceleration',\n",
       "                                                                          'year']),\n",
       "                                                                        ('categorical',\n",
       "                                                                         Pipeline(steps=[('onehotencoder',\n",
       "                                                                                          OneHotEncoder(sparse=False))]),\n",
       "                                                                         ['origin'])])),\n",
       "                                       ('ridge', Ridge())]),\n",
       "             param_grid={'ridge__alpha': array([  0.1       ,   0.14384499,   0.20691381,   0.29763514,\n",
       "         0.42813324,   0.61584821,   0.88586679,   1.27427499,\n",
       "         1.83298071,   2.6366509 ,   3.79269019,   5.45559478,\n",
       "         7.8475997 ,  11.28837892,  16.23776739,  23.35721469,\n",
       "        33.59818286,  48.32930239,  69.51927962, 100.        ])},\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "779a2a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': 2.636650898730358}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13761651",
   "metadata": {},
   "source": [
    "### Evaluating the MSE on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53175a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6c71a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.86825679928795"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, lasso_cv.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e062cadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.812101791027336"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, ridge_cv.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d233a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "winner = ridge_cv.best_estimator_.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

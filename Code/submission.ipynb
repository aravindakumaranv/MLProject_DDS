{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Hotel Project - Team Indecision Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Members:\n",
    "* Aravinda Kumaran Venkadeswaran&emsp;&ensp;&nbsp;(442274)\n",
    "* Bhargav Sridhar&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(441060)\n",
    "* Kanthimathinathan Anantharaman&emsp;&ensp;&nbsp;(441058)\n",
    "* Shreaya Arun Kumar&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(441059)\n",
    "* Vaidehi Kulkarni&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;(447266)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "* [Problem Statement](#problem-statement)\n",
    "* [The Code](#code)\n",
    "    * [Necessary Imports](#necessary-imports)\n",
    "    * [Reading Data](#reading-data)\n",
    "    * [Exploratory Data Analysis](#exploratory-data-analysis)\n",
    "    * [Preprocessing](#preprocessing)\n",
    "        * [Pipeline](#pipeline)\n",
    "        * [Data Splitting](#data-splitting)\n",
    "        * [Correlation Matrix](#correlation-matrix)\n",
    "    * [Classification](#classification)\n",
    "        * [SVM](#svm-model)\n",
    "            * [Defining the Model](#defining-the-model-svm)\n",
    "            * [Performing hyperparameter tuning](#performing-hyperparameter-tuning-svm)\n",
    "            * [Plotting ROC curve](#plotting-roc-curve-svm)\n",
    "        * [Logistic Regression](#logistic-regression-model)\n",
    "            * [Defining the Model](#defining-the-model-logistic-regression)\n",
    "            * [Feature Selection by Backward Enumeration](#feature-selection-by-backward-enumeration-logistic-regression)\n",
    "            * [Performing hyperparameter tuning](#performing-hyperparameter-tuning-logisitic-regression)\n",
    "            * [Plotting ROC curve](#plotting-roc-curve-logistic-regression)\n",
    "        * [Refitting Best Classification Model](#refitting-best-classification-model)\n",
    "    * [Regression](#regression)\n",
    "        * [Linear Regression](#linear-regression)\n",
    "            * [Defining the Model](#defining-the-model-linear-regression)\n",
    "            * [Feature Selection by Backward Enumeration](#feature-selection-by-backward-enumeration-linear-regression)\n",
    "            * [Performing hyperparameter tuning](#performing-hyperparameter-tuning-linear-regression)\n",
    "            * [Testing different Linear Regression models](#testing-different-linear-regression-models)\n",
    "        * [Polynomial Regression](#polynomial-regression)\n",
    "        * [Refitting Best Regression Model](#refitting-best-regression-model)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement <a class=\"anchor\" id=\"problem-statement\"></a>\n",
    "Given extensive information on around 120000 hotel bookings, create a binary classification model to predict whether a booking will be cancelled. In the case of cancelled bookings, predict how many days in advance the guest cancels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Code <a class=\"anchor\" id=\"code\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Imports <a class=\"anchor\" id=\"necessary-imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import svm\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_squared_error, r2_score, roc_curve, auc, classification_report\n",
    "from numpy.typing import ArrayLike\n",
    "from typing import List\n",
    "import math\n",
    "from utility import *\n",
    "import pickle as pkl\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data <a class=\"anchor\" id=\"reading-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis <a class=\"anchor\" id=\"exploratory-data-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_bookings = pd.value_counts(data[\"country\"]).iloc[:10]\n",
    "top_10_countries = data[data[\"country\"].isin(list(top_10_bookings.index))]\n",
    "country_based_group = top_10_countries.groupby(\"is_canceled\").country.value_counts()\n",
    "country_names = list(top_10_bookings.index)\n",
    "df_country = pd.DataFrame(\n",
    "    dict(\n",
    "        Country = country_names + country_names,\n",
    "        isCanceled = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        CategorySize = list(country_based_group.values),\n",
    "    )\n",
    ")\n",
    "\n",
    "s0=df_country.query('isCanceled==0')\n",
    "s1=df_country.query('isCanceled==1')\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "df_country=df_country['Country']\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x = s0['Country'], \n",
    "        y = s0['CategorySize'],\n",
    "        name = 'Not Canceled'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x = s1['Country'], \n",
    "        y = s1['CategorySize'],\n",
    "        name = 'Canceled'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(    \n",
    "    title=\"Canceled and Not Canceled bookings per Country (Top 10)\", \n",
    "    barmode='group', \n",
    "    xaxis_title=\"Country\", \n",
    "    yaxis_title=\"Count of Bookings\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows top 10 countries grouped based on the total count of bookings. Portugal has the highest booking and cancellation rate, since the hotels are based out of Portugal.\n",
    "\n",
    "From this list, cancellation rate for its neighbours (GBR to DEU) is close to 20% with bookings in the range of 7000 to 10000.\n",
    "\n",
    "Countries ranking 6-10 on the graph comprise of other European countries and Brazil (possible popularity due to use of a common language). These countries have bookings in the range of 1500-3800 with a cancellation rate of approximately 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = data['arrival_date_year'].unique().tolist()\n",
    "years.sort()\n",
    "x_ticks = []\n",
    "booking_counts = []\n",
    "cancellation_counts = []\n",
    "months = ['January','February','March','April','May','June','July','August','September','October','November','December']\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        x_ticks.append(month + \" \" + str(year))\n",
    "        booking_counts.append(len(data[(data['arrival_date_year'] == year) & (data['arrival_date_month'] == month)]))\n",
    "        cancellation_counts.append(len(data[(data['arrival_date_year'] == year) & (data['arrival_date_month'] == month) & (data['is_canceled'] == 1)]))\n",
    "\n",
    "# Removing time periods that are out of range of the data\n",
    "x_ticks = x_ticks[6:-4]\n",
    "booking_counts = booking_counts[6:-4]\n",
    "cancellation_counts = cancellation_counts[6:-4]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_ticks, \n",
    "        y=booking_counts, \n",
    "        name=\"Bookings\"\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_ticks, \n",
    "        y=cancellation_counts, \n",
    "        name=\"Cancellations\"\n",
    "    )\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Booking and Cancellation Counts over Time\", \n",
    "    xaxis_title=\"Time\", \n",
    "    yaxis_title=\"Count\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the above graph that the cancellations line maintains a similar trend as the bookings line. This means that the proportion of cancellations has remained more or less the same throughout the time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = data['hotel'].unique()\n",
    "booking_counts = []\n",
    "cancellation_counts = []\n",
    "for hotel in hotels:\n",
    "    booking_counts.append(len(data[(data['hotel'] == hotel)]))\n",
    "    cancellation_counts.append(len(data[(data['hotel'] == hotel) & (data['is_canceled'] == 1)]))\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=hotels,\n",
    "        y=booking_counts,\n",
    "        name=\"Bookings\",\n",
    "        width=0.5\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=hotels,\n",
    "        y=cancellation_counts,\n",
    "        name=\"Cancellations\",\n",
    "        width=0.3\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Booking and Cancellation Counts against Hotel\", \n",
    "    xaxis_title=\"Hotel\", \n",
    "    yaxis_title=\"Count\",\n",
    "    barmode=\"overlay\"\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total bookings in the given time period of a City Hotel is nearly twice as that of Resort Hotel.\n",
    "Cancellation rate of City Hotel bookings is approx. 1.5 times as that of the Resort Hotel bookings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sunburst graph\n",
    "fig = px.sunburst(data, path=['deposit_type', 'is_canceled'], labels=[\"a\",\"b\",\"a\",\"b\",\"c\",\"d\"])\n",
    "for i in range(len(fig.data[0].labels)):\n",
    "    if(fig.data[0].labels[i] == '0'):\n",
    "        fig.data[0].labels[i] = 'Not Canceled'\n",
    "    if(fig.data[0].labels[i] == '1'):\n",
    "        fig.data[0].labels[i] = 'Canceled'\n",
    "fig.update_traces(    \n",
    "    textinfo=\"label+percent parent\"\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Cancellations grouped by Deposit Type\", \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above depicts the cancellations by deposit types. Bookings with refundable deposits are a very small percentage of the total bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram between No. of special requests and cancellation\n",
    "ratio = {}\n",
    "\n",
    "for i in data['total_of_special_requests'].unique():\n",
    "    ratio[i] = len(data[(data['total_of_special_requests']==i) & (data['is_canceled']==1)]) / len(data[data['total_of_special_requests']==i])\n",
    "\n",
    "reqd_df = pd.DataFrame({\"total_of_special_requests\": ratio.keys(), \"Cancellation ratio\": ratio.values()})\n",
    "fig = px.bar(reqd_df, x=\"total_of_special_requests\", y=\"Cancellation ratio\", width=1000, labels={'total_of_special_requests':'Total No. of Special Requests'})\n",
    "fig.update_traces(width=0.5)\n",
    "fig.update_layout(\n",
    "    title=\"Cancellation rates grouped by No. of special requests\", \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the above graph that cutomers are less likely to cancel when they have more special requests associated with their booking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.sunburst(data, path=['customer_type', 'is_canceled'])\n",
    "for i in range(len(fig.data[0].labels)):\n",
    "    if(fig.data[0].labels[i] == '0'):\n",
    "        fig.data[0].labels[i] = 'Not Canceled'\n",
    "    if(fig.data[0].labels[i] == '1'):\n",
    "        fig.data[0].labels[i] = 'Canceled'\n",
    "fig.update_layout(\n",
    "    title=\"Cancellations grouped by Customer Type\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows cancellations by customer type. The cancellation percentage is highest for transient customers(i.e. bookings which are standalone not associated with other bookings) with 40.7%.<br>\n",
    "1.Cancellation rates for other categories:<br>\n",
    "2.Transient party (Booking associated with at least one other transient booking) - 25.4 %<br>\n",
    "3.Contract (Booking has an allotment or other type of contract associated to it) - 31.0 %<br>\n",
    "4.Group (Booking is associated with a group)                                     -  0.1 %<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.sunburst(data, path=['market_segment', 'is_canceled'])\n",
    "for i in range(len(fig.data[0].labels)):\n",
    "    if(fig.data[0].labels[i] == '0'):\n",
    "        fig.data[0].labels[i] = 'Not Canceled'\n",
    "    if(fig.data[0].labels[i] == '1'):\n",
    "        fig.data[0].labels[i] = 'Canceled'\n",
    "fig.update_layout(\n",
    "    title=\"Cancellations grouped by Market Segment\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows cancellations by market segment. Nearly half the bookings were made through online travel agents with the highest cancellation rates coming from group bookings ~61%.<br>\n",
    "Cancellation percentages for other categories:<br>\n",
    "1.Online TA(Travel agents):- 37 %<br>\n",
    "2.Offline TA/TO(Travel Agents/Tour Operators):- 34.3 %<br>\n",
    "3.Direct:- 15 %<br>\n",
    "4.Corporate:- 18.7 %<br>\n",
    "5.Aviation:- 21.95 %<br>\n",
    "6.Complementary:- 13.05%<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing <a class=\"anchor\" id=\"preprocessing\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline <a class=\"anchor\" id=\"pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(\n",
    "    FunctionTransformer(DataImputer(column_to_value={\"children\":0})),\n",
    "    FunctionTransformer(ArrivalDateTransformer()),\n",
    "    FunctionTransformer(ColumnRemover(['country','agent','company','month','day','distribution_channel'])),\n",
    "    FunctionTransformer(RoomTypeTransformer()),\n",
    "    FunctionTransformer(MealTypeTransformer()),\n",
    "    FunctionTransformer(UncleanDataPointsRemover()),\n",
    "    FunctionTransformer(CancellationsDaysInserter()),\n",
    "    FunctionTransformer(ColumnRemover(['reservation_status','reservation_status_date','arrival_date']))\n",
    ")\n",
    "\n",
    "pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Matrix <a class=\"anchor\" id=\"correlation-matrix\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting <a class=\"anchor\" id=\"data-splitting\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test sets (80% train, 20% test)\n",
    "data_train_and_validation, data_test = train_test_split(data, test_size=0.2, stratify=data['is_canceled'])\n",
    "\n",
    "# Splitting data into features and label for classification\n",
    "X_train_and_validation = data_train_and_validation.drop(['is_canceled','cancellation_days'], axis='columns')\n",
    "y_train_and_validation = data_train_and_validation.is_canceled\n",
    "\n",
    "X_test = data_test.drop(['is_canceled','cancellation_days'], axis='columns')\n",
    "y_test = data_test.is_canceled\n",
    "\n",
    "# Regression \n",
    "X_train_and_validation_reg = data_train_and_validation[data_train_and_validation['is_canceled']==1]\n",
    "X_train_and_validation_reg = X_train_and_validation_reg.drop(['is_canceled','cancellation_days'], axis='columns')\n",
    "y_train_and_validation_reg = data_train_and_validation[data_train_and_validation['is_canceled']==1]\n",
    "y_train_and_validation_reg = y_train_and_validation_reg.cancellation_days\n",
    "\n",
    "X_test_reg = data_test[data_test['is_canceled']==1]\n",
    "X_test_reg = X_test_reg.drop(['is_canceled','cancellation_days'], axis='columns')\n",
    "y_test_reg = data_test[data_test['is_canceled']==1]\n",
    "y_test_reg = y_test_reg.cancellation_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the entire dataset\n",
    "# Splitting data into features and label for classification\n",
    "X_data = data.drop(['is_canceled','cancellation_days'], axis='columns')\n",
    "X_data_reg = data[data['is_canceled']==1]\n",
    "X_data_reg = data.drop(['is_canceled','cancellation_days'], axis='columns')\n",
    "\n",
    "y_data = data.is_canceled\n",
    "y_data_reg = data[data['is_canceled']==1]\n",
    "y_data_reg = data.cancellation_days"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification <a class=\"anchor\" id=\"classification\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train_and_validation.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM <a class=\"anchor\" id=\"svm-model\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the model <a class=\"anchor\" id=\"defining-the-model-svm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SVM_classifier(features):\n",
    "    \n",
    "    # Deposit Type, Customer Type, Hotel and Market Segment will be one hot encoded\n",
    "    categorical_features = list(\n",
    "        set(features) & set(\n",
    "            [\n",
    "                'deposit_type',\n",
    "                'customer_type',\n",
    "                'hotel',\n",
    "                'market_segment',\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    numerical_features = list(set(features) - set(categorical_features))\n",
    "\n",
    "    columnTransformer = ColumnTransformer(\n",
    "        transformers = [\n",
    "            (\n",
    "                'categorical', OneHotEncoder(\n",
    "                    handle_unknown=\"ignore\"\n",
    "                ), \n",
    "                categorical_features\n",
    "            ),\n",
    "            (\n",
    "                'numerical', \n",
    "                StandardScaler(), \n",
    "                numerical_features\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    classifier = make_pipeline(\n",
    "        columnTransformer,\n",
    "        svm.SVC(probability=True)\n",
    "    )\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performing hyperparameter tuning <a class=\"anchor\" id=\"performing-hyperparameter-tuning-svm\"></a>\n",
    "\n",
    "C -  essentially a regularisation parameter, which controls the trade-off between achieving a hyperplane with the largest minimum margin, and a hyperplane that correctly separates as many instances as possible. We decided to check with the values 1, 20 and 100. \n",
    "\n",
    "Kernel - Linear and RBF\n",
    "\n",
    "RandomizedSearchCV instead of GridSearchCV to minimize compute time and due to resource constraints. Trying 2 iterations on the any of the combinations above to equip our model with the best hyperparameter with a k-cross-fold of value 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_SVM_classifier(features)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'svc__C':[1,20,100],\n",
    "        'svc__kernel':['linear', 'rbf']\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_pipeline_SVM = RandomizedSearchCV(pipeline, param_grid, cv=5, n_iter=2, verbose=100, return_train_score = False)\n",
    "grid_pipeline_SVM.fit(X=X_train_and_validation, y=y_train_and_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_pipeline_SVM.cv_results_)[['param_svc__kernel','param_svc__C','mean_test_score','rank_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_SVM = grid_pipeline_SVM.best_params_\n",
    "best_model_SVM = grid_pipeline_SVM.best_estimator_\n",
    "predictions = best_model_SVM.predict(X_test)\n",
    "plot_confusion_matrix(y_test, predictions)\n",
    "print(\"Best Score: \", grid_pipeline_SVM.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC Curve <a class=\"anchor\" id=\"plotting-roc-curve-svm\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_model_SVM.decision_function(X_train_and_validation)    \n",
    "y_test_pred = best_model_SVM.decision_function(X_test) \n",
    "train_fpr, train_tpr, tr_thresholds = roc_curve(y_train_and_validation, y_train_pred)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label=\" AUC TRAIN =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\" AUC TEST =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.plot([0,1],[0,1],'g--')\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC(ROC curve)\")\n",
    "plt.grid(color='black', linestyle='-', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Cancelled\", \"Not Cancelled\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Model <a class=\"anchor\" id=\"logisitic-regression-model\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the model <a class=\"anchor\" id=\"defining-the-model-logistic-regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR_classifier(features):\n",
    "    \n",
    "    # Deposit Type, Customer Type, Hotel and Market Segment will be one hot encoded\n",
    "    categorical_features = list(\n",
    "        set(features) & set(\n",
    "            [\n",
    "                'deposit_type',\n",
    "                'customer_type',\n",
    "                'hotel',\n",
    "                'market_segment',\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    numerical_features = list(set(features) - set(categorical_features))\n",
    "\n",
    "    columnTransformer = ColumnTransformer(\n",
    "        transformers = [\n",
    "            (\n",
    "                'categorical', \n",
    "                OneHotEncoder(\n",
    "                    handle_unknown=\"ignore\"\n",
    "                ),\n",
    "                categorical_features\n",
    "            ),\n",
    "            (\n",
    "                'numerical',\n",
    "                StandardScaler(),\n",
    "                numerical_features\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    classifier = make_pipeline(\n",
    "        columnTransformer,\n",
    "        LogisticRegression(\n",
    "            max_iter=100000\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return classifier\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection by Backward Enumeration <a class=\"anchor\" id=\"feature-selection-by-backward-enumeration-logistic-regression\"></a>\n",
    "This gave us features that did not result in good predictions and so we decided to not use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "class Solution:\n",
    "    features: List[int]\n",
    "    mse: float\n",
    "\n",
    "    def __init__(self, y: ArrayLike):\n",
    "        self.features = list()\n",
    "        self.mse = mean_squared_error(y, [y.mean()]*len(y))\n",
    "\n",
    "    def update(self, features: List[int], mse: float) -> bool:\n",
    "        if(mse < self.mse):\n",
    "            print(', '.join(str(x) for x in features))\n",
    "            print(f\"\\tNew Error: {mse:.3f} better than {self.mse:.3f}\")\n",
    "            self.features = features\n",
    "            self.mse = mse\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "all_features = features\n",
    "current_features = all_features\n",
    "best = Solution(y_train_and_validation)\n",
    "while len(current_features)>0:\n",
    "    selected_feature = None\n",
    "\n",
    "    for feature in current_features:\n",
    "        new_features = current_features.drop(feature)\n",
    "        Xr = X_train_and_validation[new_features]\n",
    "        mses = cross_val_score(estimator=get_LR_classifier(new_features), X=Xr, y=y_train_and_validation, cv=kfold, scoring=\"neg_mean_squared_error\")\n",
    "        mse = -np.average(mses)\n",
    "\n",
    "        if(best.update(features=new_features, mse=mse)):\n",
    "            selected_feature = feature\n",
    "        \n",
    "    if(selected_feature):\n",
    "        current_features = current_features.drop(selected_feature)\n",
    "    else:\n",
    "        break\n",
    "best_features = list(best.features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performing hyperparameter tuning <a class=\"anchor\" id=\"performing-hyperparameter-tuning-logistic-regression\"></a>\n",
    "\n",
    "C -  essentially a regularisation parameter, which controls the trade-off between achieving a hyperplane with the largest minimum margin, and a hyperplane that correctly separates as many instances as possible. We decided to check with the values 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000. \n",
    "\n",
    "Solver - Newton-cg, lbfgs and liblinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_LR_classifier(features)\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'logisticregression__C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000],\n",
    "        'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_pipeline = GridSearchCV(pipeline,param_grid)\n",
    "grid_pipeline.fit(X=X_train_and_validation, y=y_train_and_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logistic_regression_params = grid_pipeline.best_params_\n",
    "best_logistic_regression_model = grid_pipeline.best_estimator_\n",
    "predictions = best_logistic_regression_model.predict(X_test)\n",
    "plot_confusion_matrix(y_test, predictions)\n",
    "print(\"Best Score: \", grid_pipeline.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_pipeline.best_estimator_.fit(X_data,y_data)\n",
    "pkl.dump(grid_pipeline.best_estimator_,open(\"logistic_regression.p\",\"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting ROC Curve <a class=\"anchor\" id=\"plotting-roc-curve-logistic-regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_logistic_regression_model.decision_function(X_train_and_validation)    \n",
    "y_test_pred = best_logistic_regression_model.decision_function(X_test) \n",
    "train_fpr, train_tpr, tr_thresholds = roc_curve(y_train_and_validation, y_train_pred)\n",
    "test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(train_fpr, train_tpr, label=\" AUC TRAIN =\"+str(auc(train_fpr, train_tpr)))\n",
    "plt.plot(test_fpr, test_tpr, label=\" AUC TEST =\"+str(auc(test_fpr, test_tpr)))\n",
    "plt.plot([0,1],[0,1],'g--')\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"AUC(ROC curve)\")\n",
    "plt.grid(color='black', linestyle='-', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Cancelled\", \"Not Cancelled\"]\n",
    "print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refitting best classification model <a class=\"anchor\" id=\"refitting-best-classification-model\"></a>\n",
    "We have chosen the best SVM model since it performs better than the best Logistic Regression model.\n",
    "We will now train it with the entire dataset for production level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_SVM.fit(X_data,y_data)\n",
    "pkl.dump(best_model_SVM,open(\"classification_model.p\",\"wb\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression <a class=\"anchor\" id=\"regression\"></a>\n",
    "We tried with Simple Linear Regression, Lasso and Ridge Regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X_train_and_validation_reg.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression <a class=\"anchor\" id=\"linear-regression\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining the model <a class=\"anchor\" id=\"defining-the-model-linear-regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressor(features, type):\n",
    "    # Deposit Type, Customer Type, Hotel and Market Segment will be one hot encoded\n",
    "    categorical_features = list(set(features) & set([\n",
    "        'deposit_type',\n",
    "        'customer_type',\n",
    "        'hotel',\n",
    "        'market_segment',\n",
    "    ]))\n",
    "    numerical_features = list(set(features) - set(categorical_features))\n",
    "\n",
    "    columnTransformer = ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('categorical', OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "            ('numerical', StandardScaler(), numerical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    regressor = make_pipeline(\n",
    "        columnTransformer,\n",
    "        type\n",
    "    )\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection by Backward Enumeration <a class=\"anchor\" id=\"feature-selection-by-backward-enumeration-linear-regression\"></a>\n",
    "This gave us features that resulted in slightly worse R2 scores so we decided not to make use of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "class Solution:\n",
    "    features: List[int]\n",
    "    mse: float\n",
    "\n",
    "    def __init__(self, y: ArrayLike):\n",
    "        self.features = list()\n",
    "        self.mse = mean_squared_error(y, [y.mean()]*len(y))\n",
    "\n",
    "    def update(self, features: List[int], mse: float) -> bool:\n",
    "        if(mse < self.mse):\n",
    "            print(', '.join(str(x) for x in features))\n",
    "            print(f\"\\tNew Error: {mse:.3f} better than {self.mse:.3f}\")\n",
    "            self.features = features\n",
    "            self.mse = mse\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "all_features = features\n",
    "current_features = all_features\n",
    "best = Solution(y_train_and_validation_reg)\n",
    "while len(current_features)>0:\n",
    "    selected_feature = None\n",
    "\n",
    "    for feature in current_features:\n",
    "        new_features = current_features.drop(feature)\n",
    "        Xr = X_train_and_validation_reg[new_features]\n",
    "        mses = cross_val_score(estimator=get_regressor(new_features, Ridge()), X=Xr, y=y_train_and_validation_reg, cv=kfold, scoring=\"neg_mean_squared_error\")\n",
    "        mse = -np.average(mses)\n",
    "\n",
    "        if(best.update(features=new_features, mse=mse)):\n",
    "            selected_feature = feature\n",
    "        \n",
    "    if(selected_feature):\n",
    "        current_features = current_features.drop(selected_feature)\n",
    "    else:\n",
    "        break\n",
    "best_features = list(best.features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Performing hyperparameter tuning <a class=\"anchor\" id=\"performing-hyperparameter-tuning-linear-regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_alpha = np.logspace(start=-3, stop=0, num=20)\n",
    "ridge_alpha = np.logspace(start=-1, stop=2, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_estimator=get_regressor(features,LinearRegression())\n",
    "\n",
    "lasso_cv = GridSearchCV(\n",
    "    estimator=get_regressor(features,Lasso(max_iter=10000)),\n",
    "    param_grid={\n",
    "        'lasso__alpha': lasso_alpha\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error')\n",
    "\n",
    "ridge_cv = GridSearchCV(\n",
    "    estimator=get_regressor(features,Ridge()),\n",
    "    param_grid={\n",
    "        'ridge__alpha': ridge_alpha\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_estimator.fit(X_train_and_validation_reg, y_train_and_validation_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_cv.fit(X_train_and_validation_reg, y_train_and_validation_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_cv.fit(X_train_and_validation_reg, y_train_and_validation_reg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing different linear regression models (R^2 and MSE scores) <a class=\"anchor\" id=\"testing-different-linear-regression-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Regression Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 with training data\n",
    "print(\"R2 with training data: \",r2_score(y_train_and_validation_reg, linear_regression_estimator.predict(X_train_and_validation_reg)))\n",
    "print(\"Mean Squared Error: \", math.sqrt(mean_squared_error(y_train_and_validation_reg, linear_regression_estimator.predict(X_train_and_validation_reg))))\n",
    "\n",
    "# R2 with testing data\n",
    "print(\"R2 with testing data\",r2_score(y_test_reg, linear_regression_estimator.predict(X_test_reg)))\n",
    "print(\"Mean Squared Error: \", math.sqrt(mean_squared_error(y_test_reg, linear_regression_estimator.predict(X_test_reg))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regularization Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 with training data\n",
    "print(\"R2 with training data: \",r2_score(y_train_and_validation_reg, lasso_cv.predict(X_train_and_validation_reg)))\n",
    "print(\"Mean Squared Error: \", math.sqrt(mean_squared_error(y_train_and_validation_reg, lasso_cv.predict(X_train_and_validation_reg))))\n",
    "\n",
    "# R2 with testing data\n",
    "print(\"R2 with testing data\",r2_score(y_test_reg, lasso_cv.predict(X_test_reg)))\n",
    "print(\"Mean Squared Error: \", math.sqrt(mean_squared_error(y_test_reg, lasso_cv.predict(X_test_reg))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regularization Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 with training data\n",
    "print(\"R2 with training data: \",r2_score(y_train_and_validation_reg, ridge_cv.predict(X_train_and_validation_reg)))\n",
    "print(\"Mean Squared Error: \", math.sqrt(mean_squared_error(y_train_and_validation_reg, ridge_cv.predict(X_train_and_validation_reg))))\n",
    "\n",
    "# R2 with testing data\n",
    "print(\"R2 with testing data\",r2_score(y_test_reg, ridge_cv.predict(X_test_reg)))\n",
    "print(\"Mean Squared Error: \", math.sqrt(mean_squared_error(y_test_reg, ridge_cv.predict(X_test_reg))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial regression <a class=\"anchor\" id=\"polynomial-regression\"></a>\n",
    "We are not using this model since we achieve better results with the linear regression models and our data may not be polynomial in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_poly_regressor(features):\n",
    "    # Deposit Type, Customer Type, Hotel and Market Segment will be one hot encoded\n",
    "    categorical_features = list(set(features) & set([\n",
    "        'deposit_type',\n",
    "        'customer_type',\n",
    "        'hotel',\n",
    "        'market_segment',\n",
    "    ]))\n",
    "    numerical_features = list(set(features) - set(categorical_features))\n",
    "\n",
    "    columnTransformer = ColumnTransformer(\n",
    "        transformers = [\n",
    "            ('categorial', OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "            ('numerical', StandardScaler(), numerical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    poly_regressor = make_pipeline(\n",
    "        columnTransformer,\n",
    "        PolynomialFeatures(degree= 2),\n",
    "        LinearRegression(),\n",
    "            \n",
    "    )\n",
    "    \n",
    "    return poly_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_regressor = get_poly_regressor(features)\n",
    "poly_regressor.fit(X_train_and_validation_reg, y_train_and_validation_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 with training data\n",
    "print(\"R2 with training data: \",r2_score(y_train_and_validation_reg, poly_regressor.predict(X_train_and_validation_reg)))\n",
    "print(\"Mean Squared Error: \", math.sqrt(mean_squared_error(y_train_and_validation_reg, poly_regressor.predict(X_train_and_validation_reg))))\n",
    "\n",
    "# R2 with testing data\n",
    "print(\"R2 with testing data\",r2_score(y_test_reg, poly_regressor.predict(X_test_reg)))\n",
    "print(\"Mean Squared Error: \", math.sqrt(mean_squared_error(y_test_reg, poly_regressor.predict(X_test_reg))))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refitting best regression model <a class=\"anchor\" id=\"refitting-best-regression-model\"></a>\n",
    "\n",
    "1. We have chosen the Simple Linear Regression as our final model because we can see that all 3 models give similar scores and for the sake of interpretability we decided to go with Simple Linear Regression. \n",
    "\n",
    "2. We also noticed from our Correlation Matrix that our data does not suffer from multicollinearity nor do the R2 values show any signs of overfitting, which did not call for Regularization in our case. \n",
    "\n",
    "3. We will now train it with the entire dataset for production level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "linear_regression_estimator.fit(X_data_reg,y_data_reg)\n",
    "pkl.dump(linear_regression_estimator,open(\"regression.p\",\"wb\"))"
=======
    "ridge_cv.best_estimator_.fit(X_data_reg,y_data_reg)\n",
    "pkl.dump(ridge_cv.best_estimator_,open(\"regression_model.p\",\"wb\"))"
>>>>>>> origin
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7678789a7c5556a9e37c786e19b4dcfdffe90c67947640637d5b08bebb96e06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
